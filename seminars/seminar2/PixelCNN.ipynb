{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://github.com/pilipolio/learn-pytorch/blob/master/201708_ToyPixelCNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_as_image(binary_image, figsize=(10, 5)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(binary_image, cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, utils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal or masked convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(width, height, starting_point):\n",
    "    row_grid, col_grid = np.meshgrid(np.arange(width), np.arange(height), indexing='ij')\n",
    "    # TODO: change the mask\n",
    "    mask = np.ones((width, height))\n",
    "    return mask\n",
    "\n",
    "def conv_mask(width, height, include_center=False):\n",
    "    return 1.0 * causal_mask(width, height, starting_point=(width//2, height//2 + include_center - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-ing all inputs weights after center point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_channels, in_channels, width, height = 2, 2, 3, 3\n",
    "conv_weights = 1 + np.arange(out_channels * in_channels * width * height).reshape((out_channels, in_channels, width, height))\n",
    "\n",
    "masked_weights = conv_weights * conv_mask(width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        _, n_channels, width, height = self.weight.size()\n",
    "\n",
    "        mask = conv_mask(width, height, include_center=mask_type=='B')\n",
    "        self.register_buffer('mask', torch.from_numpy(mask).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    n_channels = 4\n",
    "    kernel_size = 7\n",
    "    padding = 3\n",
    "    n_pixels_out = 2 # binary 0/1 pixels\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            MaskedConv2d('A', in_channels=1, out_channels=self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=self.n_channels, out_channels=self.n_pixels_out, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pixel_logits = self.layers(x)\n",
    "        return pixel_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/pixelcnn_logits.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple generative model of LCD digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CELL_LENGTH = 4\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT = 2 * CELL_LENGTH + 5, CELL_LENGTH + 4\n",
    "\n",
    "def vertical_stroke(rightness, downness):\n",
    "    \"\"\"\n",
    "    Return a 2d numpy array representing an image with a single vertical stroke in it.\n",
    "    `rightness` and `downness` are values from [0, 1] and define the position of the vertical stroke.\n",
    "    \"\"\"\n",
    "    i = (downness * (CELL_LENGTH + 1)) + 2\n",
    "    j = rightness * (CELL_LENGTH + 1) + 1\n",
    "    x = np.zeros(shape=(IMAGE_WIDTH, IMAGE_HEIGHT), dtype=np.float64)\n",
    "    x[i + np.arange(CELL_LENGTH), j] = 1.\n",
    "    return x\n",
    "\n",
    "def horizontal_stroke(downness):\n",
    "    \"\"\"\n",
    "    Analogue to vertical_stroke, but it returns horizontal strokes.\n",
    "    `downness` is here a value in [0, 1, 2].\n",
    "    \"\"\"\n",
    "    i = (downness * (CELL_LENGTH + 1)) + 1\n",
    "    x = np.zeros(shape=(IMAGE_WIDTH, IMAGE_HEIGHT), dtype=np.float64)\n",
    "    x[i, 2 + np.arange(CELL_LENGTH)] = 1.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALkAAAEhCAYAAAAj7wHGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAADYElEQVR4nO3cwY3CMBBA0Qyij+2/A9rhTg/eBlikCALL13vHJIc5fFmWJWfWWhuUnT49ABxN5OSJnDyRkydy8nZFPjOXowaBZzxqc/YcIc6M80b+rbXW3Htuu0KeyMkTOXkiJ0/k5ImcPJGTJ3LyRE6eyMkTOXkiJ0/k5ImcPJGTJ3LyRE6eyMkTOXnnTw9wBL++e62Zu1cnv4aVnDyRkydy8kROnsjJEzl5IidP5OSJnDyRkydy8kROnsjJEzl5IidP5OSJnDyRkydy8kROXvIi87dfvOW1rOTkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETp7IyRM5eSInT+TkiZw8kZMncvJETt555/e3bduuRwwCT/r568Wstd45CLyd7Qp5IidP5OSJnDyRkydy8kROnsjJEzl5v4CUFaJhLuFiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_as_image(horizontal_stroke(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-603990977eed>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  DIGITS_STROKES = np.array([[0, 2, 3, 4, 5, 6], [5, 6], [0, 1, 2, 4, 5], [0, 1, 2, 5, 6], [1, 3, 5, 6], [0, 1, 2, 3, 6], [0, 1, 2, 3, 4, 6], [0, 5, 6], np.arange(7), [0, 1, 2, 3, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "BASE_STROKES = np.asarray(\n",
    "    [horizontal_stroke(k) for k in range(3)] + [vertical_stroke(k, l) for k in range(2) for l in range(2)])\n",
    "\n",
    "DIGITS_STROKES = np.array([[0, 2, 3, 4, 5, 6], [5, 6], [0, 1, 2, 4, 5], [0, 1, 2, 5, 6], [1, 3, 5, 6], [0, 1, 2, 3, 6], [0, 1, 2, 3, 4, 6], [0, 5, 6], np.arange(7), [0, 1, 2, 3, 5, 6]])\n",
    "\n",
    "def random_digits(strokes=BASE_STROKES, digit_as_strokes=DIGITS_STROKES, fixed_label=None):\n",
    "    label = fixed_label if fixed_label is not None else np.random.choice(len(digit_as_strokes))\n",
    "    combined_strokes = strokes[digit_as_strokes[label], :, :].sum(axis=0)\n",
    "    return combined_strokes, label\n",
    "\n",
    "def batch_images_to_one(batches_images):\n",
    "    n_square_elements = int(np.sqrt(batches_images.shape[0]))\n",
    "    rows_images = np.split(np.squeeze(batches_images), n_square_elements)\n",
    "    return np.vstack([np.hstack(row_images) for row_images in rows_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class LcdDigits(Dataset):\n",
    "\n",
    "    def __init__(self, n_examples):\n",
    "        digits, labels = zip(*[random_digits() for _ in range(n_examples)])\n",
    "        self.digits = np.asarray(digits, dtype=np.float64)\n",
    "        self.labels = np.asarray(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        digit_with_channel = self.digits[idx][np.newaxis, :, :]\n",
    "        \n",
    "        return torch.from_numpy(digit_with_channel).float(), torch.from_numpy(np.array([self.labels[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "train_dataset = LcdDigits(BATCH_SIZE * 2)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: look at the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 0.7626\n",
      "Epoch [2/25], Loss: 0.2961\n",
      "Epoch [3/25], Loss: 0.1066\n",
      "Epoch [4/25], Loss: 0.0467\n",
      "Epoch [5/25], Loss: 0.0385\n",
      "Epoch [6/25], Loss: 0.0343\n",
      "Epoch [7/25], Loss: 0.0321\n",
      "Epoch [8/25], Loss: 0.0312\n",
      "Epoch [9/25], Loss: 0.0301\n",
      "Epoch [10/25], Loss: 0.0298\n",
      "Epoch [11/25], Loss: 0.0255\n",
      "Epoch [12/25], Loss: 0.0255\n",
      "Epoch [13/25], Loss: 0.0248\n",
      "Epoch [14/25], Loss: 0.0251\n",
      "Epoch [15/25], Loss: 0.0247\n",
      "Epoch [16/25], Loss: 0.0246\n",
      "Epoch [17/25], Loss: 0.0246\n",
      "Epoch [18/25], Loss: 0.0245\n",
      "Epoch [19/25], Loss: 0.0245\n",
      "Epoch [20/25], Loss: 0.0244\n",
      "Epoch [21/25], Loss: 0.0243\n",
      "Epoch [22/25], Loss: 0.0243\n",
      "Epoch [23/25], Loss: 0.0243\n",
      "Epoch [24/25], Loss: 0.0243\n",
      "Epoch [25/25], Loss: 0.0244\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "N_EPOCHS = 25\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.005\n",
    "\n",
    "cnn = PixelCNN()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "\n",
    "train_dataset = LcdDigits(BATCH_SIZE * 50)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(input=cnn(images), target=torch.squeeze(images).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Loss: %.4f' %(epoch+1, N_EPOCHS, loss.detach().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequentially generating new samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(n_samples, starting_point=(0, 0), starting_image=None):\n",
    "\n",
    "    samples = torch.from_numpy(\n",
    "        starting_image if starting_image is not None else np.zeros((n_samples * n_samples, 1, IMAGE_WIDTH, IMAGE_HEIGHT))).float()\n",
    "\n",
    "    cnn.train(False)\n",
    "    probs_samples = np.stack(2*[np.zeros_like(samples)], axis=-1)\n",
    "    \n",
    "    for i in range(IMAGE_WIDTH):\n",
    "        for j in range(IMAGE_HEIGHT):\n",
    "            if i < starting_point[0] or (i == starting_point[0] and j < starting_point[1]):\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                out = cnn(Variable(samples))\n",
    "            probs = F.softmax(out[:, :, i, j], dim=-1).data\n",
    "            samples[:, :, i, j] = torch.multinomial(probs, 1).float()\n",
    "            probs_samples[:, :, i, j] = probs.numpy()[:, None, :]\n",
    "            \n",
    "    return samples.numpy(), probs_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samples, probs_samples = generate_samples(n_samples=10)\n",
    "show_as_image(batch_images_to_one(samples), figsize=(10, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating from precalculated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 10\n",
    "starting_point = (4, 3)\n",
    "\n",
    "mask = causal_mask(IMAGE_WIDTH, IMAGE_HEIGHT, starting_point)\n",
    "\n",
    "starting_images = digits_list = [random_digits(fixed_label=d)[0] for d in range(10)]\n",
    "batch_starting_images = np.expand_dims(np.stack([i * mask for i in starting_images] * n_images), axis=1)\n",
    "\n",
    "samples, probs_samples = generate_samples(n_images, starting_image=batch_starting_images, starting_point=starting_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_as_image(np.hstack([(1 + mask) * i for i in starting_images]), figsize=(10, 10))\n",
    "\n",
    "show_as_image(\n",
    "    batch_images_to_one((samples * (1 + mask))),\n",
    "    figsize=(10, 20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
