{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rUp2xWkdGFu"
   },
   "source": [
    "# Autoregressive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zk6rWePvdGFv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io\n",
    "import itertools\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-wuVhhNdGFz"
   },
   "source": [
    "Use the following functions to train your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U3MMoLe6dGFz"
   },
   "outputs": [],
   "source": [
    "def get_cross_entropy_loss(model, x):\n",
    "    # we advice you firstly understand the 1st task\n",
    "    # and then return to this code\n",
    "    # your code\n",
    "    \n",
    "def train_epoch(model, train_loader, optimizer, use_cuda):\n",
    "    model.train()\n",
    "  \n",
    "    train_losses = []\n",
    "    for x in train_loader:\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        loss = get_cross_entropy_loss(model, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, use_cuda):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x in data_loader:\n",
    "            if use_cuda:\n",
    "                x = x.cuda()\n",
    "            loss = get_cross_entropy_loss(model, x)\n",
    "            total_loss += loss * x.shape[0]\n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "    return avg_loss.item()\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs, lr, use_cuda=False):\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = [eval_model(model, test_loader, use_cuda)]\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses.extend(train_epoch(model, train_loader, optimizer, use_cuda))\n",
    "        test_loss = eval_model(model, test_loader, use_cuda)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ak7kqNGfdGF2"
   },
   "source": [
    "## Task 1 (3 points): 1D histogram\n",
    "\n",
    "In this task you should train histogram model on discrete 1D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HafgFHSodGF3"
   },
   "outputs": [],
   "source": [
    "def generate_1d_data(count, bins):\n",
    "    np.random.seed(42)\n",
    "    a = 0.2 + 0.05 * np.random.randn(count)\n",
    "    b = 0.6 + 0.15 * np.random.randn(count)\n",
    "    mask = np.random.rand(count) < 0.5\n",
    "    samples = (a * mask + b * (1 - mask)) * (bins - 1)\n",
    "    data = np.clip(samples.astype('int'), 0.0, (bins - 1))\n",
    "    split = int(0.8 * count)\n",
    "    train_data, test_data = data[:split], data[split:]\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def plot_1d_data(train_data, test_data):\n",
    "    bins = int(max(test_data.max(), train_data.max()) - min(test_data.min(), train_data.min())) + 1\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1.set_title('train')\n",
    "    ax1.hist(train_data, bins=bins, density=True)\n",
    "    ax1.set_xlabel('x')\n",
    "    ax2.set_title('test')\n",
    "    ax2.hist(test_data, bins=np.arange(bins), density=True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_training_curves(train_losses, test_losses):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    n_epochs = len(test_losses) - 1\n",
    "    x_train = np.linspace(0, n_epochs, len(train_losses))\n",
    "    x_test = np.arange(n_epochs + 1)\n",
    "\n",
    "    plt.plot(x_train, train_losses, label='train loss')\n",
    "    plt.plot(x_test, test_losses, label='test loss')\n",
    "    plt.legend()\n",
    "    plt.title('training curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('NLL')\n",
    "    \n",
    "    \n",
    "def plot_1d_distribution(data, distribution):\n",
    "    size = len(distribution)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(data, bins=np.arange(size) - 0.5, label='train data', density=True)\n",
    "\n",
    "    x = np.linspace(-0.5, size - 0.5, 1000)\n",
    "    y = distribution.repeat(1000 // size)\n",
    "    plt.plot(x, y, label='learned distribution')\n",
    "\n",
    "    plt.title('learned distribution')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "Ri9wZ3ZAdGF5",
    "outputId": "148e500c-4bbe-4a83-be63-09dceb4db197"
   },
   "outputs": [],
   "source": [
    "BINS = 100\n",
    "COUNT = 5000\n",
    "\n",
    "train_data, test_data = generate_1d_data(COUNT, BINS)\n",
    "plot_1d_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFSdE1E8dGF9"
   },
   "source": [
    "Let $\\theta = (\\theta_0, \\dots, \\theta_{d}) \\in \\mathbb{R}^{d}$ are model parameters. Your model is a softmax function: $p(k| \\theta) = \\frac{e^{\\theta_k}}{\\sum_{l=1}^d e^{\\theta_{l}}}$.\n",
    "\n",
    "The goal is to train $p(k| \\theta)$ using maximum likelihood via stochastic gradient descent on the training set, using $\\theta$ initialized to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJwGQ_SsdGF-"
   },
   "outputs": [],
   "source": [
    "class SoftmaxModel(nn.Module):\n",
    "    def __init__(self, bins):\n",
    "        super().__init__()\n",
    "        self.bins = bins\n",
    "        self.logits = nn.Parameter(torch.zeros(bins))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # your code\n",
    "        \n",
    "        \n",
    "def get_distribution(model):\n",
    "    # your code (apply softmax to model logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that your model outputs right shape\n",
    "model = SoftmaxModel(BINS)\n",
    "assert [10, BINS] == list(model(torch.rand((10, 2))).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 791
    },
    "colab_type": "code",
    "id": "ZdPLQARbdGGA",
    "outputId": "d2f18699-b030-4c7d-fd18-9b400aa27e35"
   },
   "outputs": [],
   "source": [
    "# you have to choose these parameters by yourself\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "USE_CUDA = \n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "train_losses, test_losses = train_model(model, train_loader, test_loader, epochs=EPOCHS, lr=LR, use_cuda=USE_CUDA)\n",
    "\n",
    "# check that your loss is not too large\n",
    "assert test_losses[-1] < 4.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = get_distribution(model)\n",
    "\n",
    "# check that function get_distribution works correctly\n",
    "assert isinstance(distribution, np.ndarray)\n",
    "assert distribution.shape == (BINS,)\n",
    "\n",
    "plot_training_curves(train_losses, test_losses)\n",
    "plot_1d_distribution(train_data, distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4GLIbwqkdGGD"
   },
   "source": [
    "## Task 2 (5 points): MADE on 2d data\n",
    "\n",
    "Train MADE model on single image (https://arxiv.org/abs/1502.03509).\n",
    "\n",
    "You will work with bivariate data of the form $x = (x_0,x_1)$, where $x_0, x_1 \\in \\{0, \\dots, \\text{n_bins}\\}$. Implement and train a MADE model through maximum likelihood to represent $p(x_0, x_1)$ on the given image, with any autoregressive ordering of your choosing ($p(x_0, x_1) = p(x_0)p(x_1 | x_0)$ or $p(x_0, x_1) = p(x_1)p(x_0 | x_1)$). We advice you first of all think about what conditional dictribution you want to fit and how MADE's masks should look like. It may be useful to one-hot encode your inputs.\n",
    "\n",
    "You do not have to change these functions (except the path to the data file, download it from here: https://drive.google.com/file/d/1GUthJrA5fBpvi593Swo36t8zaFw9Dyak/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ngmqoNA2dGGD"
   },
   "outputs": [],
   "source": [
    "def generate_2d_data(count, bins):\n",
    "    # change the path to the image\n",
    "    im = Image.open(\n",
    "        os.path.join('drive', 'My Drive', 'DGM2020', 'homework_supplementary', 'dgm.png')\n",
    "    ).resize((bins, bins)).convert('L')\n",
    "    im = np.array(im).astype('float32')\n",
    "    dist = im / im.sum()\n",
    "\n",
    "    pairs = list(itertools.product(range(bins), range(bins)))\n",
    "    idxs = np.random.choice(len(pairs), size=count, replace=True, p=dist.reshape(-1))\n",
    "    samples = np.array([pairs[i] for i in idxs])\n",
    "\n",
    "    split = int(0.8 * len(samples))\n",
    "    return dist, samples[:split], samples[split:]\n",
    "\n",
    "\n",
    "def plot_2d_data(train_data, test_data):\n",
    "    bins = int(max(test_data.max(), train_data.max()) - min(test_data.min(), train_data.min())) + 1\n",
    "    train_dist, test_dist = np.zeros((bins, bins)), np.zeros((bins, bins))\n",
    "    for i in range(len(train_data)):\n",
    "        train_dist[train_data[i][0], train_data[i][1]] += 1\n",
    "    train_dist /= train_dist.sum()\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        test_dist[test_data[i][0], test_data[i][1]] += 1\n",
    "    test_dist /= test_dist.sum()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n",
    "    ax1.set_title('Train Data')\n",
    "    ax1.imshow(train_dist, cmap='gray')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_xlabel('x1')\n",
    "    ax1.set_ylabel('x0')\n",
    "\n",
    "    ax2.set_title('Test Data')\n",
    "    ax2.imshow(test_dist, cmap='gray')\n",
    "    ax2.axis('off')\n",
    "    ax2.set_xlabel('x1')\n",
    "    ax2.set_ylabel('x0')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_2d_distribution(true_dist, learned_dist):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "    ax1.imshow(true_dist, cmap='gray')\n",
    "    ax1.set_title('True Distribution')\n",
    "    ax1.axis('off')\n",
    "    ax2.imshow(learned_dist, cmap='gray')\n",
    "    ax2.set_title('Learned Distribution')\n",
    "    ax2.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "eX5p_02jdGGG",
    "outputId": "76fbb984-7071-4944-80dc-a4dd6e8af425"
   },
   "outputs": [],
   "source": [
    "COUNT = 20000\n",
    "BINS = 60\n",
    "\n",
    "image, train_data, test_data = generate_2d_data(COUNT, BINS)\n",
    "plot_2d_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R--RMmRgdGGI"
   },
   "outputs": [],
   "source": [
    "def to_one_hot(labels, d):\n",
    "    # one hot encoding\n",
    "    one_hot = torch.FloatTensor(labels.shape[0], d).to(labels.device)\n",
    "    one_hot.zero_()\n",
    "    one_hot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "class MaskedLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.register_buffer('mask', torch.ones(out_features, in_features))\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.mask.data.copy_(torch.from_numpy(mask.astype(np.uint8).T))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.mask * self.weight, self.bias)\n",
    "\n",
    "    \n",
    "class MADE(nn.Module):\n",
    "    def __init__(self, nin, bins, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self.nin = nin\n",
    "        self.nout = nin * bins\n",
    "        self.bins = bins\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        # we will use the trivial ordering of input units\n",
    "        self.ordering = np.arange(self.nin)\n",
    "\n",
    "        # define a simple MLP neural net\n",
    "        # you can define a list of modules (fc + relu) and stack it using nn.Sequential\n",
    "        self.net = # your code\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "        self.create_mask()  # builds the initial self.m connectivity\n",
    "\n",
    "        \n",
    "    def create_mask(self):\n",
    "        self.masks = []\n",
    "        # your code\n",
    "        self.masks = # your code\n",
    "\n",
    "        # set the masks in all MaskedLinear layers\n",
    "        layers = [l for l in self.net.modules() if isinstance(l, MaskedLinear)]\n",
    "        for l, m in zip(layers, masks):\n",
    "            l.set_mask(m)\n",
    "\n",
    "    def visualize_masks(self):\n",
    "        for m in self.masks:\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.imshow(m, cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # your code (do not forget to use one hot encoding)\n",
    "\n",
    "    def sample(self, n, use_cuda=True):\n",
    "        xs = []\n",
    "        for _ in range(n):\n",
    "            # generate random object\n",
    "            x = \n",
    "            if use_cuda:\n",
    "                x = x.cuda()\n",
    "            for it in range(self.nin):\n",
    "                # autoregressive generation is sequential\n",
    "                # generate probs to the generated sample\n",
    "                probs = \n",
    "                distr = torch.distributions.categorical.Categorical(probs)\n",
    "                # assign the sampled component to the corresponding element of the x\n",
    "                x[0, it] = \n",
    "            xs.append(x)\n",
    "        xs = torch.cat(xs)\n",
    "        return xs.cpu().numpy()\n",
    "\n",
    "\n",
    "def get_distribution(model, use_cuda=True):\n",
    "    # returns an array BINS x BINS with probabilities of each pixel\n",
    "    # you have to generate grid of all possible image positions,\n",
    "    # take the model output for this grid\n",
    "    # take the corresponding elements of the output tensor\n",
    "    # reshape if to the square matrrix\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZES = \n",
    "\n",
    "model = MADE(2, BINS, HIDDEN_SIZES)\n",
    "assert [10, 60, 2] == list(model(torch.randint(0, BINS, (10, 2))).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show on your masks and assure that they are correct\n",
    "model.visualize_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_NWRhm_zdGGK",
    "outputId": "15baafb5-b110-4899-9824-64fb7903b2c7"
   },
   "outputs": [],
   "source": [
    "# you have to choose these parameters by yourself\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "USE_CUDA = \n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "train_losses, test_losses = train_model(model, train_loader, test_loader, epochs=EPOCHS, lr=LR, use_cuda=USE_CUDA)\n",
    "\n",
    "assert test_losses[-1] < 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = get_distribution(model, use_cuda=USE_CUDA)\n",
    "assert distribution.shape == (BINS, BINS)\n",
    "\n",
    "plot_training_curves(train_losses, test_losses)\n",
    "plot_2d_distribution(image, distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw samples from model \n",
    "\n",
    "samples = model.sample(5000)\n",
    "plot_2d_data(train_data, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (5 points): MADE on MNIST\n",
    "\n",
    "\n",
    "You do not have to change this functions (except the path to the data file, download it from here: https://drive.google.com/file/d/1Ms-RBybrueI3_w2CRj7lM9mYjfvFRL6w/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    train_data = data['train'].astype('float32')[:, :, :, 0] > 128\n",
    "    test_data = data['test'].astype('float32')[:, :, :, 0] > 128\n",
    "    train_data = train_data.astype('uint8').reshape(-1, 28 * 28)\n",
    "    test_data = test_data.astype('uint8').reshape(-1, 28 * 28)\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def show_samples(samples, title, nrow=10):\n",
    "    samples = torch.FloatTensor(samples).reshape(-1, 28, 28)\n",
    "    samples = torch.unsqueeze(samples, axis=1)\n",
    "    grid_img = make_grid(samples, nrow=nrow)\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_data(data, title):\n",
    "    idxs = np.random.choice(len(data), replace=False, size=(100,))\n",
    "    images = train_data[idxs]\n",
    "    show_samples(images, title)\n",
    "\n",
    "# change the path to the file\n",
    "train_data, test_data = load_pickle(os.path.join('drive', 'My Drive', 'DGM2020', 'homework_supplementary', 'mnist.pkl'))\n",
    "visualize_data(train_data, 'MNIST samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZES = \n",
    "\n",
    "model = MADE(28 * 28, 2, HIDDEN_SIZES)\n",
    "assert [10, 2, 28 * 28] == list(model(torch.randint(0, 2, (10, 28 * 28))).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show on your masks and assure that they are correct\n",
    "model.visualize_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you have to choose these parameters by yourself\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "USE_CUDA = \n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "train_losses, test_losses = train_model(model, train_loader, test_loader, epochs=EPOCHS, lr=LR, use_cuda=USE_CUDA)\n",
    "\n",
    "assert test_losses[-1] < 0.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = model.sample(25)\n",
    "show_samples(samples, title='MNIST samples', nrow=5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "hw1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
